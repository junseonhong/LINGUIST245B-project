---
title: "Replication of Bias in polar questions by Domaneschi, F., Romero, M., and Braun, B. (2017, Glossa)"
author: "Junseon Hong (junseonh@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

The original study "Bias in polar questions: Evidence from British English and German production experiments" by Domaneschi, Romero, and Braun (2017) investigated how different types of bias — original speaker bias (prior beliefs) and contextual evidence bias (information from the context) — influence the form of polar questions used in British English and German. They examined four key question types: positive polar questions (PosQ), low negation polar questions (LowNQ), high negation polar questions (HiNQ), and polar questions with "really" (really-PosQ). Through production experiments, they found that speakers select different forms depending on the interaction between the two kinds of bias.

Building on the original study, this replication explored whether American English speakers demonstrate similar sensitivity to original speaker bias and contextual evidence bias when selecting different polar question forms. Like the original study, this replication manipulated original bias (for p, neutral, against p) and contextual evidence (for p, neutral, against p) across various conversational scenarios. Participants selected and produced the polar question form they found most appropriate among PosQ, LowNQ, HiNQ, really-PosQ, or an "Other" option.

## Methods

### Power Analysis

To achieve 80% power to detect the original effect size observed in the neutral/p condition, which is the weakest effect in the study, approximately 83 participants are required:

pwr.t.test(d = 0.312, sig.level = 0.05, power = 0.80, type = "one.sample", alternative = "two.sided")

              n = 82.57147
              d = 0.312
      sig.level = 0.05
          power = 0.8
    alternative = two.sided
    
This sample size is relatively high and seems unfeasible.
Alternatively, in the ¬p/p condition, which had the second weakest effect, about 17 participants are needed to achieve 80% power:

pwr.t.test(d = 0.718, sig.level = 0.05, power = 0.80, type = "one.sample", alternative = "two.sided")

              n = 17.24403
              d = 0.718
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

### Planned Sample

Planned sample size was 25, including any who fail attention checks or eligibility criteria. Participants were native speakers of American English.

### Materials

Original article used 46 written scenarios, including one practice trial, 30 target trials, and 15 filler trials. These scenarios presented ordinary fictional conversations (e.g., two friends preparing dinner, two students looking for the library). Each story was composed of two caption/picture pairs designed to manipulate original speaker bias and contextual evidence bias. After reading each scenario, participants selected the most appropriate polar question from a set of options.

This replication study employed the same materials and design framework, but with a reduced number of scenarios.


### Procedure	

The procedure used in this replication study precisely followed that of the original research as below:

"There were six experimental lists, rotating the relevant levels of original bias and contextual evidence across trials following a Latin Square design. Consequently, each participant received each of the 30 experimental items, but each item appeared in only one of the six conditions (resulting in 5 items per condition). The trials were pseudo-randomized, repeating a certain condition at most once. Each list further included all the filler items, approximately evenly distributed throughout the list. The practice trial was placed at the beginning of the list. Participants were randomly assigned to one of the experimental lists (7 participants for each list).

In all trials, the first caption was shown on the screen together with the first picture, whose purpose was to generate a positive, negative, or neutral original bias toward the proposition p. To proceed, participants had to press the space bar on the keyboard. The second caption was then shown on the screen together with the second picture, whose purpose was to generate a positive, negative, or contextual evidence bias toward the proposition p. After pressing the space bar, a list of questions appeared on the screen. After producing a question, participants could proceed by pressing the space bar again. Their response was recorded directly."

### Analysis Plan

The replication followed the analysis procedures of the original study, which is presented below:

"Participant responses were manually segmented, and the initial online coding was independently verified by a second coder. Minor variations in word order (e.g., lack of verb inversion), tense, number agreement, or the presence of discourse particles were disregarded if they did not affect the interpretation of the speaker's bias toward the proposition.

Statistical analysis focused on whether a specific polar question form was chosen in the majority of cases (i.e., more than 50%) within each bias condition. To assess this, the percentage of participants selecting the most frequent question type was averaged both by participants (t₁) and by items (t₂). These means were then submitted to two separate one-sample t-tests against the 50% baseline to determine if the preferred form was selected significantly more often than would be expected by chance."

### Differences from Original Study

While the original study considered 6 conditions, replication considered 7 conditions including ¬p/p condition (negative speaker original bias and positive contextual evidence bias) which was not considered in original study.

### Methods Addendum (Post Data Collection)

#### Actual Sample
A total of 92 samples were collected (2 from the pilot study and 90 from the main experiment). All participants were native speakers of American English.

#### Differences from pre-data collection methods plan
None

## Results

### Data preparation

Data preparation following the analysis plan.

#### Load Relevant Libraries and Functions
```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(janitor)
library(ggplot2)
library(knitr)
library(purrr)
library(patchwork)
library(magick)
```

#### Import data
```{r load-data}
data_path <- "https://junseonhong.github.io/LINGUIST245B-project/final_data.csv"

df.data <- read_csv(data_path) %>% 
  clean_names()

#View(df.data)
```

#### Prepare data for analysis - create columns etc.
```{r table-summary}
# Count responses by condition and PQ type
table_raw <- df %>%
  group_by(bias_condition, selected_question_type) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(bias_condition) %>%
  mutate(percentage = 100 * count / sum(count))

table <- table_raw %>%
  select(bias_condition, selected_question_type, percentage) %>%
  pivot_wider(
    names_from = selected_question_type,
    values_from = percentage,
    values_fill = 0
  )

head(table)
```

```{r plot-with-ci, message=F, warning=F}
# Define order of conditions
condition_order <- c("n/n", "n/¬p", "p/n", "p/¬p", "n/p", "¬p/p", "¬p/n")

# Define custom colors for question types
pq_colors <- c(
  "HNQ" = "#191970",        # dark navy
  "LNQ" = "#87CEEB",        # light blue
  "Other" = "grey60",       # grey
  "PPQ" = "#FFC0CB",        # light pink
  "ReallyPQ" = "#DC143C"    # crimson red
)

# Recode bias labels and create condition factor
df <- df %>%
  mutate(
    original_bias = recode(caption1_condition,
                           "spk-neu" = "n",
                           "spk-pos" = "p",
                           "spk-neg" = "¬p"),
    contextual_bias = recode(caption2_condition,
                             "evi-neu" = "n",
                             "evi-pos" = "p",
                             "evi-neg" = "¬p"),
    bias_condition = paste(original_bias, contextual_bias, sep = "/"),
    bias_condition = factor(bias_condition, levels = condition_order)
  )

# Item-level proportions
item_props <- df %>%
  group_by(category, bias_condition, selected_question_type) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(category, bias_condition) %>%
  mutate(prop = count / sum(count)) %>%
  ungroup()

# Summarize with mean and 95% CI
pq_summary <- item_props %>%
  group_by(bias_condition, selected_question_type) %>%
  summarise(
    mean_prop = mean(prop),
    se = sd(prop) / sqrt(n()),
    ci_low = mean_prop - 1.96 * se,
    ci_high = mean_prop + 1.96 * se,
    .groups = "drop"
  ) %>%
  mutate(
    mean_pct = mean_prop * 100,
    ci_low_pct = ci_low * 100,
    ci_high_pct = ci_high * 100,
    bias_condition = factor(bias_condition, levels = condition_order),
    selected_question_type = factor(selected_question_type, levels = names(pq_colors))
  )

# Plot with error bars
ggplot(pq_summary, aes(x = bias_condition, y = mean_pct, fill = selected_question_type)) +
  geom_col(position = position_dodge(width = 0.9), width = 0.8, color = "black") +
  geom_errorbar(
    aes(ymin = ci_low_pct, ymax = ci_high_pct),
    position = position_dodge(width = 0.9),
    width = 0.2,
    linewidth = 0.6
  ) +
  scale_fill_manual(
    values = pq_colors,
    labels = c("HNQ", "LNQ", "Other", "PosQ", "ReallyPosQ")
  ) +
  geom_hline(yintercept = 50, linetype = "dashed", color = "black") +
  ylim(0, 100) +
  labs(
    x = "Condition",
    y = "Percentage of occurrence",
    fill = "Polar question type"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "right",
    legend.title = element_text(face = "bold")
  )
```

### Confirmatory analysis

In the original study, the percentage of the preferred choice was averaged by participants and by items, and two separate one-sample t-tests were conducted. However, in the replication study, only the by-items analysis was conducted, as each subject saw each item only once.

```{r t_test}
# Define a function to run the t-test by item (category)
run_ttest <- function(condition, pq_type) {
  df %>%
    filter(bias_condition == condition) %>%
    group_by(category) %>%
    summarise(prop_selected = mean(selected_question_type == pq_type)) %>%
    summarise(
      condition = condition,
      pq_type = pq_type,
      mean_prop = mean(prop_selected),
      t_result = list(t.test(prop_selected, mu = 0.25))
    )
}

# Apply the function to each condition
ttest_results <- most_preferred_types %>%
  mutate(
    result = map2(bias_condition, selected_question_type, run_ttest)
  ) %>%
  unnest(result)

# Extract t-values and p-values
summary_table <- ttest_results %>%
  mutate(
    t_value = map_dbl(t_result, ~ .x$statistic),
    df = map_dbl(t_result, ~ .x$parameter),
    p_value = map_dbl(t_result, ~ .x$p.value),
    mean_pct = round(mean_prop * 100, 1),
    sig = ifelse(p_value < 0.05, "*", "")
  ) %>%
  select(
    `preferred_choice` = pq_type,
    `Mean %` = mean_pct,
    `t` = t_value,
    `df`,
    `p` = p_value,
    `Sig.` = sig
  )
head(summary_table)
```

*Side-by-side graph with original graph is ideal here*
```{r graph-combine-remote, message=F, warning=F}
library(magick)

img1 <- image_read("https://raw.githubusercontent.com/junseonhong/LINGUIST245B-project/5d1f16c34edc26ba25d9ef65fc35db0f1eadc1ca/figure/original_fig.png")
img2 <- image_read("https://raw.githubusercontent.com/junseonhong/LINGUIST245B-project/5d1f16c34edc26ba25d9ef65fc35db0f1eadc1ca/figure/replication_fig.png")

combined <- image_append(c(img1, img2))
print(combined)
```

```{r figures, echo=FALSE, fig.show='hold', out.width='48%'}
knitr::include_graphics("https://raw.githubusercontent.com/junseonhong/LINGUIST245B-project/5d1f16c34edc26ba25d9ef65fc35db0f1eadc1ca/figure/original_fig.png")
knitr::include_graphics("https://raw.githubusercontent.com/junseonhong/LINGUIST245B-project/5d1f16c34edc26ba25d9ef65fc35db0f1eadc1ca/figure/replication_fig.png")
```

## Discussion

### Summary of Replication Attempt

n/n condition:
- Original study: the preferred PQ choice was PosQs(63%), whereas HNQs (16%) and LNQs (17%) were dispreferred.
- Replication study: the preferred PQ choice was also a PosQs(56%), whereas HNQs (22%) and LNQs (5%) were dispreferred.

p/¬p condition:
- Original study: HNQs were the preferred choice (67%) and LNQs were dispreferred (26%).
- Replication study: HNQs (45%) remained the preferred choice, while LNQs were dispreferred (14%).

comparison of HNQs the n/¬p and p/n:
- Original study: HNQs were the preferred choice in the p/n condition (65%) but dispreferred in the n/¬p condition (24%).
- Replication study: HNQs were more preferred choice in the p/n condition (38%), although the preference in the former was not as strong as in the original study.

comparing LNQs the n/¬p and p/n:
- Original study: LNQs were the preferred form in n/¬p condition (59%) but dispreferred in the p/n condition (8%).
- Replication study: LNQ were more preferred in the n/¬p condition (27%) thatn in the p/n condition (4%), but the effect of bias condition was again weaker than in the original study.

The replication study partially confirmed the findings of the original study. Overall preference patterns across bias conditions were similar, but the magnitude of the effects was consistently smaller. While the same trends in question type preferences were observed, the distinctions between conditions were less robust.

### Commentary

Analyses suggest that the observed patterns in the replication do not contradict with the original findings but are notably weaker. This may indicate that the effects reported in the original study were more sensitive to specific stimulus properties or context design than previously assumed.

The replication raises questions about the robustness of the original findings, particularly regarding the nature of the original stimuli. The materials used in the original study were not fully public and do not appear to have effectively captured the subtle contextual differences necessary to reflect positive, neutral, and negative (both types of) biases. Moreover, the consistent preference for a single question type in over 50% of responses across all conditions (despite multiple theoretically viable alternatives) was highly questionable from the outset.
